{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering analysis\n",
    "\n",
    "1. Cluster sentences of target words that appear frequently enough (More than 5 times)\n",
    "2. Cluster substitutes obtained\n",
    "\n",
    "Embeddings extracted are the one at the **11th** layer of the hidden states of `bert-base-uncased`\n",
    "\n",
    "1. **Tokenize** sentences and term\n",
    "2. Identify the token(s) of the target term, get embeddings and average if there are several -- (gay, ##est)\n",
    "3. Create **k-means clusters** of the sentences by the embeddings of the specific term\n",
    "4. Obtain clusters **metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_USED = \"RoPretrained\" #RoPretrained, #BERT\n",
    "\n",
    "# Version of analysis to create or load\n",
    "VERSION = \"avg_pca2\" # first_pca, avg_pca_vocab, first_pca_vocab\n",
    "n_clusters = range(1, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "with open('../base_dict.pkl', 'rb') as file:\n",
    "    sub_sentences = pickle.load(file)\n",
    "\n",
    "with open('../tokens_dict.pkl', 'rb') as file:\n",
    "    tokens_sentences = pickle.load(file)\n",
    "\n",
    "with open('../base_dict_noblack.pkl', 'rb') as file:\n",
    "    sub_sentences_noblack = pickle.load(file)\n",
    "\n",
    "with open('../tokens_dict_noblack.pkl', 'rb') as file:\n",
    "    tokens_sentences_noblack = pickle.load(file)\n",
    "specific_words = [tok for tok, sents  in tokens_sentences.items() if len(sents[\"sentences\"]) >= 5]\n",
    "\n",
    "# Load tokenizer and model\n",
    "if MODEL_USED == \"RoPretrained\" :\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pretrained_videogame_with_tokenizer')\n",
    "    model = AutoModel.from_pretrained('pretrained_videogame_with_tokenizer_0.001/checkpoint-3600', output_hidden_states=True)\n",
    "\n",
    "elif MODEL_USED == \"BERT\" :\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "n_components = 3\n",
    "pca = PCA(n_components=n_components)\n",
    "n_components_low = 2\n",
    "pca_low = PCA(n_components=n_components_low)\n",
    "\n",
    "split_tokens_csvpath = f\"{VERSION}/split_tokens{MODEL_USED}_{VERSION}.csv\"\n",
    "split_tokens_txtpath = f\"{VERSION}/split_tokens{MODEL_USED}_{VERSION}.txt\"\n",
    "\n",
    "target_embeddings_pkl = f\"{VERSION}/target_embeddings{MODEL_USED}_{VERSION}.pkl\"\n",
    "\n",
    "vocab_tokens_pkl = f\"vocab_tokens{MODEL_USED}.pkl\"\n",
    "similarities_for_sub_csv = f\"{VERSION}/simSub{MODEL_USED}_{VERSION}.csv\"\n",
    "\n",
    "elbow_method_plots_repeatTerms = f\"{VERSION}/elbow_method_plots_repeatTerms{MODEL_USED}_{VERSION}.pdf\"\n",
    "elbow_method_plots_substitutes = f\"{VERSION}/elbow_method_plots_substitutes{MODEL_USED}_{VERSION}.pdf\"\n",
    "\n",
    "final_results_repeatTerms = f\"{VERSION}/final_results_repeatTerms{MODEL_USED}_{VERSION}.csv\"\n",
    "final_results_substitutes = f\"{VERSION}/final_results_substitutes{MODEL_USED}_{VERSION}.csv\"\n",
    "\n",
    "clusters_repeatTerms_pkl = f\"{VERSION}/clusters_repeatTerms{MODEL_USED}_{VERSION}.pkl\"\n",
    "clusters_substitutes_pkl = f\"{VERSION}/clusters_substitutes{MODEL_USED}_{VERSION}.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(filename) :\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "def save_to_pickle(filename, item):\n",
    "    with open(filename, \"wb\") as file :\n",
    "        pickle.dump(item, file)\n",
    "\n",
    "def assign_to_dict(dic, *args):\n",
    "    for key, arg in zip(dic.keys(), args) :\n",
    "        dic[key].append(arg)\n",
    "    return dic\n",
    "\n",
    "def format_dict(original_dict, keys_to_keep):\n",
    "    return {key: original_dict[key] for key in keys_to_keep}\n",
    "\n",
    "def create_pdDf(list_of_dicts, csv_filename, save = True):\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "    df = df.apply(pd.Series.explode)\n",
    "    if save :\n",
    "        df.to_csv(csv_filename)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain embeddings, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sublist(main_list, sub_list):\n",
    "    for i in range(len(main_list) - len(sub_list) + 1):\n",
    "        if main_list[i:i + len(sub_list)] == sub_list:\n",
    "            if len(sub_list) == 1 :\n",
    "                return i\n",
    "            else :\n",
    "                return [i, i+len(sub_list)]\n",
    "    return -1\n",
    "\n",
    "def get_embeddings(sentences_for_target, contexts_with_targets, toxicity_of_target, target):\n",
    "    model.eval()\n",
    "    target_information = {\"sentence\": [], \"sentence_in_context\" : [], \"target_embeddings\" : [], \"toxicity\" : []} #before : [sentence, sentence_in_context, target_embeddings]\n",
    "    tokens_split = {\"term\": [], \"tokens\" : []}\n",
    "    print(target)\n",
    "    with torch.no_grad():\n",
    "        target_embeddings = {}\n",
    "\n",
    "        for sentence, sentence_in_context, toxicity in zip(sentences_for_target, contexts_with_targets, toxicity_of_target) :\n",
    "\n",
    "            complete_tokens = tokenizer.tokenize(sentence_in_context)\n",
    "            complete_token_ids = tokenizer.convert_tokens_to_ids(complete_tokens)\n",
    "            complete_tokens_tensor = torch.tensor([complete_token_ids])\n",
    "            outputs = model(complete_tokens_tensor)\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            complete_embeddings = hidden_states[10][0]\n",
    "\n",
    "            sentence_tokens = tokenizer.tokenize(sentence)\n",
    "            target_tokens = tokenizer.tokenize(target)\n",
    "\n",
    "            sentence_indices = find_sublist(complete_tokens, sentence_tokens)\n",
    "\n",
    "            # Possibility of different tokenization with space in front of sentence / term\n",
    "            if sentence_indices == -1 :\n",
    "                sentence_tokens = tokenizer.tokenize(\" \"+sentence)\n",
    "                sentence_indices = find_sublist(complete_tokens, sentence_tokens)\n",
    "\n",
    "            if sentence_indices != -1 :\n",
    "                sentence_embeddings = complete_embeddings[sentence_indices[0]:sentence_indices[1]]\n",
    "\n",
    "                target_indices = find_sublist(complete_tokens[sentence_indices[0]:sentence_indices[1]], target_tokens)\n",
    "                if target_indices == -1 :\n",
    "                    target_tokens = tokenizer.tokenize(\" \"+target)\n",
    "                    target_indices = find_sublist(complete_tokens[sentence_indices[0]:sentence_indices[1]], target_tokens)\n",
    "\n",
    "                if target_indices != -1 :\n",
    "                    if type(target_indices) != list :\n",
    "                        target_embeddings = sentence_embeddings[target_indices]\n",
    "                    elif type(target_indices) == list and \"avg\" in VERSION :\n",
    "                        target_embeddings = sentence_embeddings[target_indices[0]:target_indices[-1]].mean(dim=0)\n",
    "                        tokens_split[\"term\"].append(target)\n",
    "                        tokens_split[\"tokens\"].append(target_tokens)\n",
    "                    elif type(target_indices) == list and \"avg\" not in VERSION :\n",
    "                        target_embeddings = sentence_embeddings[target_indices[0]]\n",
    "                        tokens_split[\"term\"].append(target)\n",
    "                        tokens_split[\"tokens\"].append(target_tokens)\n",
    "                    assign_to_dict(target_information, sentence, sentence_in_context, target_embeddings, toxicity)\n",
    "                    \n",
    "            else :\n",
    "                print(f\"sentence : {sentence_tokens} \\n not found in context : {complete_tokens} \")\n",
    "    \n",
    "    # Token splits :\n",
    "    split_df = pd.DataFrame(tokens_split)\n",
    "    if os.path.exists(split_tokens_csvpath):\n",
    "        split_df.to_csv(split_tokens_csvpath, mode='a', header=False, index=False)\n",
    "    else :\n",
    "        split_df.to_csv(split_tokens_csvpath, mode='w', header=True, index=False)\n",
    "\n",
    "    return target_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter subword tokens\n",
    "def filter_subwords(tokens, embeddings):\n",
    "    filtered_tokens = []\n",
    "    filtered_embeddings = []\n",
    "    for token, embedding in zip(tokens, embeddings):\n",
    "        if not token.startswith(\"##\"):\n",
    "            filtered_tokens.append(token)\n",
    "            filtered_embeddings.append(embedding)\n",
    "    return filtered_tokens, np.array(filtered_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity, clustering and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate similarities\n",
    "def calculate_similarities(target_embedding, vocab_embeddings, vocab_tokens, max = False):\n",
    "    similarities = {\"token\" : [], \"similarity_score\" : [], \"embedding\" : []}\n",
    "    for i, vocab_embedding in enumerate(vocab_embeddings):\n",
    "        similarity = 1 - cosine(target_embedding, vocab_embedding)\n",
    "        if max :\n",
    "            assign_to_dict(similarities, vocab_tokens[i], similarity, vocab_embedding.reshape(1,-1))\n",
    "            \n",
    "        else : \n",
    "            assign_to_dict(similarities, vocab_tokens[i], similarity, vocab_embedding)\n",
    "\n",
    "    if max:\n",
    "        # Sort by similarity_score\n",
    "        sorted_indices = sorted(range(len(similarities[\"similarity_score\"])), key=lambda x: similarities[\"similarity_score\"][x], reverse=True)\n",
    "        sorted_similarities = {key: [similarities[key][i] for i in sorted_indices] for key in similarities}\n",
    "        return {key: sorted_similarities[key][:max] for key in sorted_similarities}\n",
    "    else:\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cluster\n",
    "def clustering(embeddings, n_clusters) :\n",
    "    kmeans = KMeans(n_clusters = n_clusters)\n",
    "    kmeans.fit_predict(embeddings)\n",
    "\n",
    "    inertia = kmeans.inertia_\n",
    "    cluster_labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    return cluster_labels, centroids, inertia   \n",
    "\n",
    "def check_possible_clusters(len_embeddings, n_clusters_list) :\n",
    "    if len_embeddings <= n_clusters_list[-1] :\n",
    "            n_clusters_list = range(1, len_embeddings)\n",
    "    return n_clusters_list\n",
    "\n",
    "# Function to identify tokens/sentences/embeddings of each cluster\n",
    "def identify(cluster_labels, cluster_id, list_of_items) :\n",
    "    return [item for item, label in zip(list_of_items, cluster_labels) if label == cluster_id]\n",
    "\n",
    "#|--------------------------------------|\n",
    "#| Functions to calculate some metrics  |\n",
    "#|--------------------------------------|\n",
    "def elbow_method(target, sentence, embeddings, n_clusters_list, pdf):\n",
    "    inertias = []\n",
    "    clusters = {}\n",
    "\n",
    "    for n in n_clusters_list:\n",
    "        cluster_labels, centroids, inertia = clustering(embeddings, n)\n",
    "        inertias.append(inertia)\n",
    "        clusters[n] = {\"cluster_labels\" : [cluster_labels], \"centroids\" : [centroids], \"inertia\" : [inertia], \"embeddings\" : [embeddings]}\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(n_clusters_list)), inertias, marker='o')\n",
    "    plt.title(f'Target : {target} - N : {n} clusters - Sentence : {sentence}')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    \n",
    "    pdf.savefig()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def calculate_variance(embeddings) :\n",
    "    return np.var(embeddings, axis=0).mean() \n",
    "\n",
    "def get_most_representative(cluster_embeddings, cluster_centroid, sentence_list):\n",
    "    min_distance = float('inf')\n",
    "    representative_item = \"\"\n",
    "    \n",
    "    for embedding, sentence in zip(cluster_embeddings, sentence_list):\n",
    "        # Calculate the distance between the embedding and the cluster centroid\n",
    "        distance = np.linalg.norm(embedding - cluster_centroid)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            representative_item = sentence\n",
    "    \n",
    "    return representative_item\n",
    "\n",
    "def get_silhouette(embeddings_list, cluster_labels) :\n",
    "    silhouette_avg = silhouette_score(embeddings_list, cluster_labels)\n",
    "    return silhouette_avg\n",
    "\n",
    "def get_max_silhouette(target_silhouettes) :\n",
    "    silhouettes = []\n",
    "    ks = []\n",
    "    for k, info in target_silhouettes.items() :\n",
    "        silhouettes.append(info[0])\n",
    "        ks.append(k)\n",
    "\n",
    "    max_silhouette = max(silhouettes)\n",
    "    index_of_max = silhouettes.index(max_silhouette)\n",
    "\n",
    "    return max_silhouette, ks[index_of_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lit(val):\n",
    "    if isinstance(val, str):\n",
    "        # Convert string representation of array to actual array\n",
    "        if \"array\" in val :\n",
    "            val = re.sub(r'array\\(|, dtype=float32\\)', '', val)\n",
    "        try:\n",
    "            # Safely evaluate the string to numpy array\n",
    "            val = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "def apply_lit_eval(df, list_of_columns) :\n",
    "    for col in list_of_columns :\n",
    "        df[col] = df[col].apply(convert_to_lit)\n",
    "    return df\n",
    "\n",
    "def generate_hover_text(row, hover_columns, index):\n",
    "    hover_text = []\n",
    "    for col in hover_columns:\n",
    "        if col in row:\n",
    "            # If the column contains a list, get the specific value for the current index\n",
    "            if isinstance(row[col], list) and len(row[col]) > index:\n",
    "                hover_text.append(f\"{col}: {row[col][index]}\")\n",
    "            else:\n",
    "                hover_text.append(f\"{col}: {row[col]}\")\n",
    "    return \"<br>\".join(hover_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_elbow(embeddings, target, sentence_to_write, n_clusters, pdf, \n",
    "                targets_info, name_of_targets_info, \n",
    "                other_infos = False, name_of_other_info = \"other\"):\n",
    "\n",
    "    embeddings_to_cluster = np.vstack(embeddings)\n",
    "    low_embeddings = pca_low.fit_transform(embeddings_to_cluster)\n",
    "    embeddings_to_cluster = pca.fit_transform(embeddings_to_cluster)\n",
    "\n",
    "    nb_of_clusters = check_possible_clusters(len(embeddings_to_cluster), n_clusters)\n",
    "\n",
    "    clusters = elbow_method(target, sentence_to_write, embeddings_to_cluster, nb_of_clusters, pdf)\n",
    "\n",
    "    clusters_results = {}\n",
    "    for nb, cluster_info in clusters.items():\n",
    "        clusters_results[nb] = {\"target_term\": [], \"sentence\": [],\"cluster_id\" : [], \n",
    "                                \"most_representative\" : [],\n",
    "                                name_of_targets_info : [], name_of_other_info :[],\n",
    "                                \"cluster_labels\" : [], \"centroids\" : [], \"inertia\" : [],\n",
    "                                \"embeddings_to_cluster\" : [], \"n_clusters\" : [],\n",
    "                                \"cluster_embeddings\" : [], \"low_embeddings\" : []}                               \n",
    "\n",
    "        for id in range(nb):\n",
    "            cluster_embeddings = identify(cluster_info[\"cluster_labels\"][0], id, embeddings_to_cluster)\n",
    "            cluster_low_embeddings = identify(cluster_info[\"cluster_labels\"][0], id, low_embeddings)\n",
    "            list_of_targets = identify(cluster_info[\"cluster_labels\"][0], id, targets_info)\n",
    "            if other_infos :\n",
    "                list_of_others = identify(cluster_info[\"cluster_labels\"][0], id, other_infos)\n",
    "            else :\n",
    "                list_of_others = False\n",
    "\n",
    "            assign_to_dict(clusters_results[nb], target, sentence_to_write, id, \n",
    "                            get_most_representative(cluster_embeddings, cluster_info[\"centroids\"], list_of_targets),\n",
    "                            list_of_targets, list_of_others, \n",
    "                            cluster_info[\"cluster_labels\"], cluster_info[\"centroids\"], cluster_info[\"inertia\"],\n",
    "                            embeddings_to_cluster, nb_of_clusters, \n",
    "                            cluster_embeddings, cluster_low_embeddings)\n",
    "\n",
    "    return clusters_results\n",
    "\n",
    "def apply_silhouette(clusters, nb_clusters) :\n",
    "    list_of_ks = []\n",
    "    list_of_scores = []\n",
    "    for possible_k in nb_clusters : \n",
    "        if possible_k == 1 : \n",
    "            continue\n",
    "\n",
    "        labels = clusters[possible_k][\"cluster_labels\"][0][0]\n",
    "        embeds = clusters[possible_k][\"embeddings_to_cluster\"][0]\n",
    "\n",
    "        score = get_silhouette(embeds, labels)\n",
    "\n",
    "        list_of_scores.append(score)\n",
    "        list_of_ks.append(possible_k)\n",
    "\n",
    "    max_silhouette = max(list_of_scores)\n",
    "    index_of_max = list_of_scores.index(max_silhouette)\n",
    "\n",
    "    return list_of_ks[index_of_max], max_silhouette\n",
    "\n",
    "def get_final_cluster(clusters, nb_clusters):\n",
    "\n",
    "    final_k, sil_score = apply_silhouette(clusters, nb_clusters)\n",
    "    final_dict = clusters[final_k]\n",
    "    final_dict[\"k\"] = [final_k]*final_k\n",
    "\n",
    "    final_dict[\"silhouette_score\"] = [sil_score]*final_k\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings = {target:get_embeddings(items[\"sentences\"], items[\"contexts\"], items[\"toxicity\"], target) for target, items in tqdm(tokens_sentences.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(target_embeddings_pkl, target_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings = open_pickle(target_embeddings_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepeatTerms - Cluster sentences for each repeated term\n",
    "\n",
    "Example :\n",
    "\n",
    "\"Gay\" : \"I'm gay\", \"he's gay\", \"this is so gay\"\n",
    "We will use the embeddings of all occurences of \"gay\" and cluster them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatTerms_results = {}\n",
    "pdf_file = elbow_method_plots_repeatTerms\n",
    "with PdfPages(pdf_file) as pdf:\n",
    "    for target, representations in tqdm(target_embeddings.items()) :\n",
    "        if target in specific_words :\n",
    "            embeddings = representations[\"target_embeddings\"]\n",
    "            sentences = representations[\"sentence\"]\n",
    "            toxicity = []\n",
    "            for sent in sentences :\n",
    "                if sent in sub_sentences.keys() :\n",
    "                    toxicity.append(sub_sentences[sent][\"toxicity\"][0])\n",
    "                    continue\n",
    "                for main_sentence, info in sub_sentences.items() :\n",
    "                    if sent in info[\"sentences\"] :\n",
    "                        toxicity.append(info[\"toxicity\"][0])\n",
    "                        break\n",
    "\n",
    "            if embeddings :\n",
    "                print(f\"There are {len(embeddings)} embeddings for the term : {target}. Clustering...\")\n",
    "                \n",
    "                repeatTerms_results[target] = apply_elbow(embeddings, target, \"all\", n_clusters,\n",
    "                                                            pdf, sentences, \"sentences\", toxicity, \"toxicity\")\n",
    "            else : \n",
    "                print(f\"SKIPPING {target} - There are {len(embeddings)} embedding(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "save_to_pickle(clusters_repeatTerms_pkl, repeatTerms_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_repeatTerms_clusters = {}\n",
    "for target, clusters in tqdm(repeatTerms_results.items()) :\n",
    "    nb_cluster = range(3, clusters[1][\"n_clusters\"][0][-1]) # same in all clusters id\n",
    "\n",
    "    final_repeatTerms_clusters[target] = get_final_cluster(clusters, nb_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_clusters = []\n",
    "columns_to_keep = [\"target_term\", \"k\", \"cluster_id\", \"sentences\", \n",
    "                    \"most_representative\", \"inertia\", \"silhouette_score\", \"toxicity\",\n",
    "                    \"cluster_embeddings\", \"low_embeddings\"]\n",
    "\n",
    "for target_term, clusters in final_repeatTerms_clusters.items():\n",
    "    list_of_clusters.append(format_dict(clusters, columns_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatTerms_df = create_pdDf(list_of_clusters, final_results_repeatTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatTerms_results = open_pickle(clusters_repeatTerms_pkl)\n",
    "repeatTerms_df = pd.read_csv(final_results_repeatTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitutes - Clustering substitutes for each main sentence\n",
    "\n",
    "\"gay for not letting us win\" : cluster all variations with substitutes. Also get similarity score and plot that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutes_results = {}\n",
    "similarities_for_subs = {}\n",
    "\n",
    "pdf_file = elbow_method_plots_substitutes\n",
    "with PdfPages(pdf_file) as pdf:\n",
    "    for main, others in tqdm(sub_sentences.items()) :\n",
    "        print(main)\n",
    "        if main == \"yeah and I'm part of the gays\" :\n",
    "            main_token = \"gAYS\"\n",
    "        else : \n",
    "            main_token = others[\"tokens\"][0]\n",
    "        print(target_embeddings[main_token][\"target_embeddings\"])\n",
    "        embedding_of_main = next(target_embeddings[main_token][\"target_embeddings\"][sent_pos] for sent_pos, sent in enumerate(target_embeddings[main_token][\"sentence\"]) if main == sent)\n",
    "        \n",
    "        embeddings_list = []\n",
    "        tokens_list = []\n",
    "        sentence_list = []\n",
    "        for sentence, token in zip(others[\"sentences\"], others[\"tokens\"][1:]) :\n",
    "            embs = next(target_embeddings[token][\"target_embeddings\"][sent_pos] for sent_pos, sent in enumerate(target_embeddings[token][\"sentence\"]) if sentence == sent)\n",
    "\n",
    "            embeddings_list.append(embs)\n",
    "            tokens_list.append(token)\n",
    "            sentence_list.append(sentence)\n",
    "        \n",
    "                    # Computing similarities\n",
    "        similarities_substitutes = calculate_similarities(embedding_of_main, embeddings_list, tokens_list)\n",
    "        assign_to_dict(similarities_substitutes, main_token, \"main_term\", embedding_of_main)\n",
    "        similarities_for_subs[main] = similarities_substitutes\n",
    "\n",
    "        if len(embeddings_list) >= n_components:\n",
    "            print(f\"There are {len(embeddings_list)} embeddings for the sentence : {main}. Clustering...\")\n",
    "\n",
    "\n",
    "            \n",
    "            similarity_sub_info = list(zip(similarities_substitutes[\"token\"], similarities_substitutes[\"similarity_score\"]))\n",
    "            substitutes_results[main] = apply_elbow(embeddings_list, main, main, n_clusters, pdf,\n",
    "                                                    sentence_list, \"sentences\", similarity_sub_info, \"tokens_and_similarity\")\n",
    "        else :\n",
    "            print(f\"not enough embeddings, {len(embeddings_list)}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "save_to_pickle(clusters_substitutes_pkl, substitutes_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(f\"{VERSION}/similarities_for_sub.pkl\", similarities_for_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_for_subs_final = {key:{'token' : value['token'], \n",
    "                                \"similarity_score\" : [round(float(score), 3)  if score != 'main_term' else 'main_term' for score in value['similarity_score']], \n",
    "                                'sorted_similarity' : sorted(zip(value['token'][:-1], [round(float(score), 3)  if score != 'main_term' else 'main_term' for score in value['similarity_score'][:-1]]), key=lambda x: x[1], reverse=True),\n",
    "                                \"token_freq\" : dict(sorted(sub_sentences[key][\"tokens_count\"][0].items(), key=lambda item: item[1], reverse=True))\n",
    "                                } \n",
    "                                \n",
    "                                for key, value in similarities_for_subs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_for_subs_df = pd.DataFrame.from_dict(similarities_for_subs_final, orient='index')\n",
    "similarities_for_subs_df.to_csv(similarities_for_sub_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other calculations and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_substitutes_clusters= {}\n",
    "for target, clusters in tqdm(substitutes_results.items()) :\n",
    "    nb_clusters = clusters[1][\"n_clusters\"][0]\n",
    "\n",
    "    final_substitutes_clusters[target] = get_final_cluster(clusters, nb_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_clusters = []\n",
    "columns_to_keep = [\"target_term\", \"k\", \"cluster_id\", \"sentences\", \"tokens_and_similarity\",\n",
    "                    \"tokens\", \"similarity_score\", \"most_representative\", \"inertia\", \"silhouette_score\", \n",
    "                    \"cluster_embeddings\", \"low_embeddings\"]\n",
    "\n",
    "for target, clusters in final_substitutes_clusters.items() :\n",
    "    clusters[\"tokens\"] = [[token for token, _ in tokens] for tokens in clusters[\"tokens_and_similarity\"]]\n",
    "    clusters[\"similarity_score\"] = [[score for _, score in tokens] for tokens in clusters[\"tokens_and_similarity\"]]\n",
    "    list_of_clusters.append(format_dict(clusters, columns_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and saves dataframe\n",
    "substitutes_df = create_pdDf(list_of_clusters, final_results_substitutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutes_results = open_pickle(clusters_substitutes_pkl)\n",
    "substitutes_df = pd.read_csv(final_results_substitutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings = open_pickle(target_embeddings_pkl)\n",
    "\n",
    "repeatTerms_results = open_pickle(clusters_repeatTerms_pkl)\n",
    "repeatTerms_df = pd.read_csv(final_results_repeatTerms)\n",
    "\n",
    "substitutes_results = open_pickle(clusters_substitutes_pkl)\n",
    "substitutes_df = pd.read_csv(final_results_substitutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XP = \"repeatTerms\" # \"repeatTerms\", \"substitutes\"\n",
    "UNIQUE_TERM = \"gay\" # False\n",
    "TOXICITY = True\n",
    "TEXT = False\n",
    "if MODEL_USED == \"RoPretrained\" :\n",
    "    MODEL = \"RoBERTa\"\n",
    "else :\n",
    "    MODEL = \"BERT\"\n",
    "\n",
    "# If df is loaded, might need to eval some columns\n",
    "df_loaded = True\n",
    "hover_columns = [\"cluster_id\", \"most_representative\"]\n",
    "columns_lit_eval = [\"cluster_embeddings\", \"low_embeddings\"]\n",
    "\n",
    "plot_3d_html = f\"{VERSION}/plot_3d_{XP}_{MODEL_USED}_{VERSION}.html\"\n",
    "plot_2d_html = f\"{VERSION}/plot_2d_{XP}_{MODEL_USED}_{VERSION}.html\"\n",
    "\n",
    "if XP == \"substitutes\" :\n",
    "    df = substitutes_df\n",
    "    main_col = \"target_term\"\n",
    "    marker_text = \"tokens\"\n",
    "    other_cols = [\"tokens\", \"similarity_score\"]\n",
    "\n",
    "elif XP == \"repeatTerms\" :\n",
    "    TOXICITY = True\n",
    "    df = repeatTerms_df\n",
    "    main_col = \"target_term\"\n",
    "    marker_text = \"sentences\"\n",
    "    other_cols = [\"sentences\", \"toxicity\"]\n",
    "\n",
    "elif XP == \"vocabSim\" :\n",
    "    df = vocabSim_df.loc[vocabSim_df['target_term'] == UNIQUE_TERM]\n",
    "    main_col = \"sentence\"\n",
    "    marker_text = \"tokens\"\n",
    "    other_cols = [\"tokens\", \"similarity_score\"]\n",
    "\n",
    "else : \n",
    "    print(\"Wrong XP name.\")\n",
    "\n",
    "# Process df\n",
    "hover_columns[0:0] = other_cols\n",
    "columns_lit_eval.extend(other_cols)\n",
    "\n",
    "if df_loaded:\n",
    "    df = apply_lit_eval(df, columns_lit_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_target_terms = df[main_col].unique()\n",
    "\n",
    "symbol_map = {\"toxic\" : \"diamond-open\", \"non-toxic\" : \"circle\", \"black\" : \"circle\"}\n",
    "\n",
    "# 3D plot overlay\n",
    "fig_3d = go.Figure()\n",
    "\n",
    "# 2D plot overlay\n",
    "fig_2d = go.Figure()\n",
    "\n",
    "# Loop through each unique target term\n",
    "for i, target_term in enumerate(unique_target_terms):\n",
    "\n",
    "    if XP == \"vocabSim\" and target_term != UNIQUE_TERM :\n",
    "        continue\n",
    "\n",
    "    subset_df = df[df[main_col] == target_term]\n",
    "\n",
    "\n",
    "    # 3D plot\n",
    "    for _, row in subset_df.iterrows():\n",
    "        cluster_id = row['cluster_id']\n",
    "    \n",
    "        x = [embed[0] for embed in row['cluster_embeddings']]\n",
    "        y = [embed[1] for embed in row['cluster_embeddings']]\n",
    "        z = [embed[2] for embed in row['cluster_embeddings']]\n",
    "        hover_info = [generate_hover_text(row, hover_columns, i) for i in range(len(x))]\n",
    "        if TEXT:\n",
    "            text_marker = [tok for tok in row[marker_text]]\n",
    "        else :\n",
    "            text_marker = None\n",
    "\n",
    "        if XP == \"repeatTerms\" :\n",
    "            symbols = [symbol_map[val] for val in row['toxicity']]\n",
    "        else : \n",
    "            symbols = None\n",
    "        fig_3d.add_trace(go.Scatter3d(\n",
    "            x=x, y=y, z=z,\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=5, color=cluster_id, colorscale='Viridis', symbol=symbols),\n",
    "            text = text_marker,\n",
    "            textposition = \"top center\",\n",
    "            textfont=dict(size=17),\n",
    "            hovertext = hover_info,\n",
    "            hoverinfo='text',\n",
    "            name=f\"Cluster {cluster_id} - {target_term}\",\n",
    "            visible=False\n",
    "        ))\n",
    "\n",
    "    # # 2D plot\n",
    "        x_low = [embed[0] for embed in row['low_embeddings']]\n",
    "        y_low = [embed[1] for embed in row['low_embeddings']]\n",
    "\n",
    "\n",
    "        fig_2d.add_trace(go.Scatter(\n",
    "            x=x_low, y=y_low,\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=15, color=cluster_id, colorscale='Viridis', symbol=symbols),\n",
    "            text = text_marker,\n",
    "            textposition = \"top center\",\n",
    "            textfont=dict(size=17),\n",
    "            hovertext = hover_info,\n",
    "            hoverinfo='text',\n",
    "            name=f\"Cluster {cluster_id} - {target_term}\",\n",
    "            visible=False\n",
    "        ))\n",
    "\n",
    "# Buttons to toggle visibility for target terms\n",
    "buttons_3d = []\n",
    "buttons_2d = []\n",
    "\n",
    "for term in unique_target_terms:\n",
    "    buttons_3d.append(dict(\n",
    "        label=f\"{term}\",\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [not trace.visible if trace.name.endswith(term) else trace.visible\n",
    "                            for trace in fig_3d.data]}, {\"title\": f\"3D {MODEL} - {term}\"}]\n",
    "    ))\n",
    "    \n",
    "    buttons_2d.append(dict(\n",
    "        label=f\"{term}\",\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [not trace.visible if trace.name.endswith(term) else trace.visible\n",
    "                            for trace in fig_2d.data]}, {\"title\": f\"2D {MODEL} - {term}\"}]\n",
    "    ))\n",
    "\n",
    "# Default visibility (none visible at the start)\n",
    "for trace in fig_3d.data:\n",
    "    trace.visible = False\n",
    "\n",
    "for trace in fig_2d.data:\n",
    "    trace.visible = False\n",
    "\n",
    "# Update layout with buttons for 3D and 2D plots\n",
    "fig_3d.update_layout(\n",
    "    title=\"3D Plot\",\n",
    "    updatemenus=[{\n",
    "        \"buttons\": buttons_3d,\n",
    "        \"direction\": \"down\",\n",
    "        \"showactive\": True,\n",
    "        \"x\": 1.3,  # Adjust the position to avoid overlap\n",
    "        \"xanchor\": \"left\",\n",
    "        \"y\": 1.05,\n",
    "        \"yanchor\": \"top\",\n",
    "    }],\n",
    "    legend_title_text=\"Cluster ID\",\n",
    "    title_x=0.05,\n",
    "    title_y=0.80,\n",
    "    title_font=dict(size=20, color=\"black\", weight=\"bold\"),\n",
    "\n",
    "    legend=dict(\n",
    "        font=dict(size=16),\n",
    "        x=1,          # Horizontal position\n",
    "        y=0.7,        # Vertical position\n",
    "        xanchor='left',   # Anchor position for x\n",
    "        yanchor='middle'  # Anchor position for y\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_2d.update_layout(\n",
    "    title=\"2D Plot\",\n",
    "    updatemenus=[{\n",
    "        \"buttons\": buttons_2d,\n",
    "        \"direction\": \"down\",\n",
    "        \"showactive\": True,\n",
    "        \"x\": 1.3,  # Adjust the position to avoid overlap\n",
    "        \"xanchor\": \"left\",\n",
    "        \"y\": 1.05,\n",
    "        \"yanchor\": \"top\",\n",
    "    }],\n",
    "    legend_title_text=\"Cluster ID\",\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title_font=dict(size=20, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "\n",
    "# Adjusting the size of the plot window to fit all buttons\n",
    "fig_3d.update_layout(height=800)\n",
    "fig_2d.update_layout(height=800)\n",
    "\n",
    "# Show the figures\n",
    "fig_3d.show()\n",
    "fig_2d.show()\n",
    "fig_2d.write_html(plot_2d_html)\n",
    "fig_3d.write_html(plot_3d_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other metrics random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksplit(df, term):\n",
    "    filt = df.loc[df[\"term\"] == term]\n",
    "    if not filt.empty :\n",
    "        return filt[\"tokens\"].values[0]\n",
    "    else : \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "freq_dict = {\"term\" : [], \"frequency\" : [], \"toxic_freq\" : [], \"nonToxic_freq\" : [], \n",
    "            \"toxic_100\": [], \"black_sub\": [], \"cluster_freq\" : [], \"clusters\" : [],\n",
    "            \"bi_clusters\" : [], \"tri_clusters\" : [], \"four_clsuters\" : [],\n",
    "            \"all_occs\" : [], \"BERT_split\": [], \"Ro_split\" : []}\n",
    "split_tokens_BERT = pd.read_csv('avg_pca2/split_tokensBERT_avg_pca2.csv')\n",
    "split_tokens_RoPretrained = pd.read_csv('avg_pca2/split_tokensRoPretrained_avg_pca2.csv')\n",
    "\n",
    "for term, info in tokens_sentences_noblack.items() :\n",
    "    print(term)\n",
    "    freq = len(info[\"sentences\"])\n",
    "    toxic_freq = len([tox for tox in info[\"toxicity\"] if tox == \"toxic\"])\n",
    "    nontoxic_freq = len([tox for tox in info[\"toxicity\"] if tox == \"non-toxic\"])\n",
    "    black_freq = len([tox for tox in info[\"toxicity\"] if tox == \"black\"])\n",
    "    try : \n",
    "        toxic_100 = round((toxic_freq/(toxic_freq+nontoxic_freq))*100, 2) # NOT CONSIDERING BLACK\n",
    "    except :\n",
    "        toxic_100 == 0\n",
    "    \n",
    "    all_ocs = info[\"sentences\"]\n",
    "    BERT_split = checksplit(split_tokens_BERT, term)\n",
    "    Ro_split = checksplit(split_tokens_RoPretrained, term)\n",
    "\n",
    "    clusters_df = substitutes_df[substitutes_df['tokens'].apply(lambda x: term in x)]\n",
    "    clusters_df = clusters_df[~clusters_df['target_term'].str.contains('black')]\n",
    "    clusters_items = {}\n",
    "    bi_counter = Counter()\n",
    "    tri_counter = Counter()\n",
    "    four_counter = Counter()\n",
    "    how_many_clusers = len(clusters_df)\n",
    "    # Loop through each row and the list within that row\n",
    "    for idx, row in clusters_df.iterrows():\n",
    "        for item in row['tokens']:\n",
    "            if item in clusters_items :\n",
    "                clusters_items[item] += 1\n",
    "            else :\n",
    "                clusters_items[item] = 1\n",
    "    \n",
    "        # Get the other items in the list besides the target term\n",
    "        other_items = [item for item in row['tokens'] if item != term]\n",
    "        \n",
    "        # Generate all possible pairs (or combinations of any size) from the other items\n",
    "        for group in itertools.combinations(other_items, 2):  # For pairs, use 2. For triples, use 3, etc.\n",
    "            bi_counter[group] += 1\n",
    "            \n",
    "        for group in itertools.combinations(other_items, 3):  # For pairs, use 2. For triples, use 3, etc.\n",
    "            tri_counter[group] += 1\n",
    "            \n",
    "        for group in itertools.combinations(other_items, 4):  # For pairs, use 2. For triples, use 3, etc.\n",
    "            four_counter[group] += 1\n",
    "    print(clusters_items)\n",
    "    clusters_items = {key: count for key, count in sorted(clusters_items.items(), key=lambda item: item[1], reverse=True)}\n",
    "    bi_counter = {key: count for key, count in sorted(bi_counter.items(), key=lambda item: item[1], reverse=True) if count > 1}\n",
    "    tri_counter = {key: count for key, count in sorted(tri_counter.items(), key=lambda item: item[1], reverse=True) if count > 1}\n",
    "    four_counter = {key: count for key, count in sorted(four_counter.items(), key=lambda item: item[1], reverse=True) if count > 1}\n",
    "\n",
    "    assign_to_dict(freq_dict, term, freq, toxic_freq, nontoxic_freq, toxic_100, black_freq, how_many_clusers,\n",
    "                    clusters_items, bi_counter, tri_counter, four_counter, all_ocs, BERT_split, Ro_split)\n",
    "\n",
    "terms_freq = pd.DataFrame(freq_dict)\n",
    "terms_freq.to_csv(f\"{VERSION}/tokens_details_of_freq.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memoire_xp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
